@article{rajkomar_scalable_2018,
	title = {Scalable and accurate deep learning with electronic health records},
	volume = {1},
	language = {en},
	number = {1},
	journal = {npj Digital Med},
	author = {Rajkomar, Alvin and Oren, Eyal and Chen, Kai and Dai, Andrew M. and Hajaj, Nissan and Hardt, Michaela and Liu, Peter J. and Liu, Xiaobing and Marcus, Jake and Sun, Mimi and Sundberg, Patrik and Yee, Hector and Zhang, Kun and Zhang, Yi and Flores, Gerardo and Duggan, Gavin E. and Irvine, Jamie and Le, Quoc and Litsch, Kurt and Mossin, Alexander and Tansuwan, Justin and Wang, De and Wexler, James and Wilson, Jimbo and Ludwig, Dana and Volchenboum, Samuel L. and Chou, Katherine and Pearson, Michael and Madabushi, Srinivasan and Shah, Nigam H. and Butte, Atul J. and Howell, Michael D. and Cui, Claire and Corrado, Greg S. and Dean, Jeffrey},
	year = {2018},
	pages = {18},
	doi={10.1038/s41746-018-0029-1}

}

@incollection{susto_time-series_2018,
	title = {Time-{Series} {Classification} {Methods}: {Review} and {Applications} to {Power} {Systems} {Data}},
	shorttitle = {Time-{Series} {Classification} {Methods}},
	doi={10.1016/b978-0-12-811968-6.00009-7},
	language = {en},
	urldate = {2022-07-14},
	booktitle = {Big {Data} {Application} in {Power} {Systems}},
	publisher = {Elsevier},
	author = {Susto, Gian Antonio and Cenedese, Angelo and Terzi, Matteo},
	year = {2018},
	pages = {179--220},
}

@article{bagnall_great_2017,
	title = {The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances},
	volume = {31},
	doi={10.1007/s10618-016-0483-9},
	shorttitle = {The great time series classification bake off},
	language = {en},
	number = {3},
	urldate = {2021-10-20},
	journal = {Data Min Knowl Disc},
	author = {Bagnall, Anthony and Lines, Jason and Bostrom, Aaron and Large, James and Keogh, Eamonn},
	year = {2017},
	pages = {606--660},
	file = {Full Text:C\:\\Users\\nd942\\Zotero\\storage\\5R7G9EKL\\Bagnall et al. - 2017 - The great time series classification bake off a r.pdf:application/pdf},
}

@article{dau_ucr_2019,
	title = {The {UCR} {Time} {Series} {Archive}},
	
	abstract = {The UCR Time Series Archive - introduced in 2002, has become an important resource in the time series data mining community, with at least one thousand published papers making use of at least one data set from the archive. The original incarnation of the archive had sixteen data sets but since that time, it has gone through periodic expansions. The last expansion took place in the summer of 2015 when the archive grew from 45 to 85 data sets. This paper introduces and will focus on the new data expansion from 85 to 128 data sets. Beyond expanding this valuable resource, this paper offers pragmatic advice to anyone who may wish to evaluate a new algorithm on the archive. Finally, this paper makes a novel and yet actionable claim: of the hundreds of papers that show an improvement over the standard baseline (1-nearest neighbor classiﬁcation), a large fraction may be mis-attributing the reasons for their improvement. Moreover, they may have been able to achieve the same improvement with a much simpler modiﬁcation, requiring just a single line of code.},
	language = {en},
	urldate = {2022-03-27},
	journal = {arXiv:1810.07758},
	author = {Dau, Hoang Anh and Bagnall, Anthony and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Keogh, Eamonn},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Dau et al. - 2019 - The UCR Time Series Archive.pdf:C\:\\Users\\nd942\\Zotero\\storage\\SRJAUP6X\\Dau et al. - 2019 - The UCR Time Series Archive.pdf:application/pdf},
}
@article{yang_10_2006,
	title = {10 {Challenging} {Problems} {In} {Data} {Mining} {Research}},
	volume = {05},
	doi={10.1142/s0219622006002258},
	abstract = {In October 2005, we took an initiative to identify 10 challenging problems in data mining research, by consulting some of the most active researchers in data mining and machine learning for their opinions on what are considered important and worthy topics for future research in data mining. We hope their insights will inspire new research efforts, and give young researchers (including PhD students) a high-level guideline as to where the hot problems are located in data mining.
            Due to the limited amount of time, we were only able to send out our survey requests to the organizers of the IEEE ICDM and ACM KDD conferences, and we received an overwhelming response. We are very grateful for the contributions provided by these researchers despite their busy schedules. This short article serves to summarize the 10 most challenging problems of the 14 responses we have received from this survey. The order of the listing does not reflect their level of importance.},
	language = {en},
	number = {04},
	urldate = {2022-07-14},
	journal = {Int. J. Info. Tech. Dec. Mak.},
	author = {Yang, Qiang and Wu, Xindong},
	year = {2006},
	pages = {597--604},}

@article{esling_time-series_2012,
	title = {Time-series data mining},
	volume = {45},
	abstract = {In almost every scientific field, measurements are performed over time. These observations lead to a collection of organized data called
              time series
              . The purpose of time-series data mining is to try to extract all meaningful knowledge from the
              shape
              of data. Even if humans have a natural capacity to perform these tasks, it remains a complex problem for computers. In this article we intend to provide a survey of the techniques applied for time-series data mining. The first part is devoted to an overview of the tasks that have captured most of the interest of researchers. Considering that in most cases, time-series task relies on the same components for implementation, we divide the literature depending on these common aspects, namely
              representation
              techniques,
              distance
              measures, and
              indexing
              methods. The study of the relevant literature has been categorized for each individual aspects. Four types of robustness could then be formalized and any kind of distance could then be classified. Finally, the study submits various research trends and avenues that can be explored in the near future. We hope that this article can provide a broad and deep understanding of the time-series data mining research field.},
	language = {en},
	number = {1},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Esling, Philippe and Agon, Carlos},
	year = {2012},
	pages = {1--34},
	
}

@article{fawaz_deep_2019,
	title = {Deep learning for time series classification: a review},
	volume = {33},
	shorttitle = {Deep learning for time series classification},
	abstract = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8,730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
	urldate = {2022-03-28},
	author = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  journal={Data mining and knowledge discovery},
  volume={33},
  number={4},
  pages={917--963},
  year={2019},
  publisher={Springer},
  doi={10.1007/s10618-019-00619-1},
}

@article{ruiz_great_2021,
	title = {The great multivariate time series classification bake off: a review and experimental evaluation of recent algorithmic advances},
	volume = {35},
	doi={10.1007/s10618-020-00727-3},
	shorttitle = {The great multivariate time series classification bake off},
	abstract = {Abstract
            Time Series Classification (TSC) involves building predictive models for a discrete target variable from ordered, real valued, attributes. Over recent years, a new set of TSC algorithms have been developed which have made significant improvement over the previous state of the art. The main focus has been on univariate TSC, i.e. the problem where each case has a single series and a class label. In reality, it is more common to encounter multivariate TSC (MTSC) problems where the time series for a single case has multiple dimensions. Despite this, much less consideration has been given to MTSC than the univariate case. The UCR archive has provided a valuable resource for univariate TSC, and the lack of a standard set of test problems may explain why there has been less focus on MTSC. The UEA archive of 30 MTSC problems released in 2018 has made comparison of algorithms easier. We review recently proposed bespoke MTSC algorithms based on deep learning, shapelets and bag of words approaches. If an algorithm cannot naturally handle multivariate data, the simplest approach to adapt a univariate classifier to MTSC is to ensemble it over the multivariate dimensions. We compare the bespoke algorithms to these dimension independent approaches on the 26 of the 30 MTSC archive problems where the data are all of equal length. We demonstrate that four classifiers are significantly more accurate than the benchmark dynamic time warping algorithm and that one of these recently proposed classifiers, ROCKET, achieves significant improvement on the archive datasets in at least an order of magnitude less time than the other three.},
	language = {en},
	number = {2},
	journal = {Data Min Knowl Disc},
	author = {Ruiz, Alejandro Pasos and Flynn, Michael and Large, James and Middlehurst, Matthew and Bagnall, Anthony},
	year = {2021},
	pages = {401--449},
}

@article{ismail_benchmarking_2020,
	title = {Benchmarking {Deep} {Learning} {Interpretability} in {Time} {Series} {Predictions}},
	
	abstract = {Saliency methods are used extensively to highlight the importance of input features in model predictions. These methods are mostly used in vision and language tasks, and their applications to time series data is relatively unexplored. In this paper, we set out to extensively compare the performance of various saliency-based interpretability methods across diverse neural architectures, including Recurrent Neural Network, Temporal Convolutional Networks, and Transformers in a new benchmark of synthetic time series data. We propose and report multiple metrics to empirically evaluate the performance of saliency methods for detecting feature importance over time using both precision (i.e., whether identified features contain meaningful signals) and recall (i.e., the number of features with signal identified as important). Through several experiments, we show that (i) in general, network architectures and saliency methods fail to reliably and accurately identify feature importance over time in time series data, (ii) this failure is mainly due to the conflation of time and feature domains, and (iii) the quality of saliency maps can be improved substantially by using our proposed two-step temporal saliency rescaling (TSR) approach that first calculates the importance of each time step before calculating the importance of each feature at a time step.},
	urldate = {2021-11-24},
	journal = {arXiv:2010.13924},
	author = {Ismail, Aya Abdelsalam and Gunady, Mohamed and Bravo, Héctor Corrada and Feizi, Soheil},
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	doi={10.48550/arXiv.2010.13924},
	file = {arXiv Fulltext PDF:C\:\\Users\\nd942\\Zotero\\storage\\E78XLXRN\\Ismail et al. - 2020 - Benchmarking Deep Learning Interpretability in Tim.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nd942\\Zotero\\storage\\HVC8SWGT\\2010.html:text/html},
}

@inproceedings{guilleme_agnostic_2019,
	address = {Portland, OR, USA},
	title = {Agnostic {Local} {Explanation} for {Time} {Series} {Classification}},
	urldate = {2021-11-10},
	doi={10.1109/ictai.2019.00067},
	booktitle = {2019 {IEEE} 31st {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	publisher = {IEEE},
	author = {Guillem{\'e}, Ma{\"e}l and Masson, V{\'e}ronique and Roz{\'e}, Laurence and Termier, Alexandre},
	year = {2019},
	pages = {432--439},
	annote = {Work section Time-Series Classification
Section interpretability},
}

@article{ates_counterfactual_2021,
	title = {Counterfactual {Explanations} for {Machine} {Learning} on {Multivariate} {Time} {Series} {Data}},
	abstract = {Applying machine learning (ML) on multivariate time series data has growing popularity in many application domains, including in computer system management. For example, recent high performance computing (HPC) research proposes a variety of ML frameworks that use system telemetry data in the form of multivariate time series so as to detect performance variations, perform intelligent scheduling or node allocation, and improve system security. Common barriers for adoption for these ML frameworks include the lack of user trust and the difficulty of debugging. These barriers need to be overcome to enable the widespread adoption of ML frameworks in production systems. To address this challenge, this paper proposes a novel explainability technique for providing counterfactual explanations for supervised ML frameworks that use multivariate time series data. The proposed method outperforms state-of-the-art explainability methods on several different ML frameworks and data sets in metrics such as faithfulness and robustness. The paper also demonstrates how the proposed method can be used to debug ML frameworks and gain a better understanding of HPC system telemetry data.},
	urldate = {2022-03-25},
	journal = {2021 International Conference on Applied Artificial Intelligence (ICAPAI)},
	author = {Ates, Emre and Aksar, Burak and Leung, Vitus J. and Coskun, Ayse K.},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {1--8},
	file = {arXiv Fulltext PDF:C\:\\Users\\nd942\\Zotero\\storage\\T26TYK8H\\Ates et al. - 2021 - Counterfactual Explanations for Machine Learning o.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nd942\\Zotero\\storage\\AWI6C3C4\\2008.html:text/html},
}

@incollection{sanchez-ruiz_instance-based_2021,
  title={Instance-based counterfactual explanations for time series classification},
  author={Delaney, Eoin and Greene, Derek and Keane, Mark T},
  doi={10.1007/978-3-030-86957-1_3},
  booktitle={{International} {Conference} on {Case-Based} {Reasoning}},
  pages={32--47},
  year={2021},
  organization={Springer}
}

@misc{meudec_raphael_tf-explain_2021,
	title = {{tf-explain}},
	copyright = {Open Access},
	
	abstract = {Interpretability Methods for tf.keras models with TensorFlow 2.x},
	urldate = {2022-05-24},
	publisher = {Zenodo},
	author = {Meudec, Raphael},
	year = {2021},
	url={https://github.com/sicara/tf-explain},
	doi = {10.5281/ZENODO.5711704},
	note={doi: 10.5281/ZENODO.5711704},
	annote = {Other
If you use tf-explain in your research, please cite it using these metadata.},
}
@article{kokhlikyan_captum_2020,
	title = {Captum: {A} unified and generic model interpretability library for {PyTorch}},
	shorttitle = {Captum},
	
	abstract = {In this paper we introduce a novel, unified, open-source model interpretability library for PyTorch [12]. The library contains generic implementations of a number of gradient and perturbation-based attribution algorithms, also known as feature, neuron and layer importance algorithms, as well as a set of evaluation metrics for these algorithms. It can be used for both classification and non-classification models including graph-structured models built on Neural Networks (NN). In this paper we give a high-level overview of supported attribution algorithms and show how to perform memory-efficient and scalable computations. We emphasize that the three main characteristics of the library are multimodality, extensibility and ease of use. Multimodality supports different modality of inputs such as image, text, audio or video. Extensibility allows adding new algorithms and features. The library is also designed for easy understanding and use. Besides, we also introduce an interactive visualization tool called Captum Insights that is built on top of Captum library and allows sample-based model debugging and visualization using feature importance metrics.},
	urldate = {2022-05-24},
	publisher = {arXiv},
	author = {Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and Reblitz-Richardson, Orion},
	year = {2020},
	journal = {arXiv:2009.07896},
	doi={10.48550/arXiv.2009.07896},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\nd942\\Zotero\\storage\\LXVQBS3J\\Kokhlikyan et al. - 2020 - Captum A unified and generic model interpretabili.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nd942\\Zotero\\storage\\E6ZRZDM5\\2009.html:text/html},
}

@article{klaise_alibi_2021,
	title = {Alibi {Explain}: {Algorithms} for {Explaining} {Machine} {Learning} {Models}},
	volume = {22},
	
	number = {181},
	journal = {Journal of Machine Learning Research},
	author = {Klaise, Janis and Looveren, Arnaud Van and Vacanti, Giovanni and Coca, Alexandru},
	year = {2021},
	pages = {1--7},
	doi={10.5555/3546258.3546439}
}

@article{siddiqui_tsviz_2019,
	title = {{TSViz}: {Demystification} of {Deep} {Learning} {Models} for {Time}-{Series} {Analysis}},
	volume = {7},
	shorttitle = {{TSViz}},
	doi={10.1109/access.2019.2912823},
	abstract = {This paper presents a novel framework for demystiﬁcation of convolutional deep learning models for time-series analysis. This is a step towards making informed/explainable decisions in the domain of time-series, powered by deep learning. There have been numerous efforts to increase the interpretability of image-centric deep neural network models, where the learned features are more intuitive to visualize. Visualization in time-series domain is much more complicated as there is no direct interpretation of the ﬁlters and inputs as compared to the image modality. In addition, little or no concentration has been devoted for the development of such tools in the domain of time-series in the past. TSViz provides possibilities to explore and analyze a network from different dimensions at different levels of abstraction which includes identiﬁcation of parts of the input that were responsible for a prediction (including per ﬁlter saliency), importance of different ﬁlters present in the network for a particular prediction, notion of diversity present in the network through ﬁlter clustering, understanding of the main sources of variation learnt by the network through inverse optimization, and analysis of the network’s robustness against adversarial noise. As a sanity check for the computed inﬂuence values, we demonstrate results regarding pruning of neural networks based on the computed inﬂuence information. These representations allow to understand the network features so that the acceptability of deep networks for time-series data can be enhanced. This is extremely important in domains like ﬁnance, industry 4.0, self-driving cars, health-care, counter-terrorism etc., where reasons for reaching a particular prediction are equally important as the prediction itself. We assess the proposed framework for interpretability with a set of desirable properties essential for any method in this direction.},
	language = {en},
	urldate = {2022-03-25},
	journal = {IEEE Access},
	author = {Siddiqui, Shoaib Ahmed and Mercier, Dominik and Munir, Mohsin and Dengel, Andreas and Ahmed, Sheraz},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Human-Computer Interaction},
	pages = {67027--67040},
	file = {Siddiqui et al. - 2019 - TSViz Demystification of Deep Learning Models for.pdf:C\:\\Users\\nd942\\Zotero\\storage\\VFD5WXLH\\Siddiqui et al. - 2019 - TSViz Demystification of Deep Learning Models for.pdf:application/pdf},
}

@inproceedings{hollig2022tsevo,
  title={TSEvo: Evolutionary Counterfactual Explanations for Time Series Classification},
  author={H{\"o}llig, Jacqueline and Kulbach, Cedric and Thoma, Steffen},
  doi={10.1109/icmla55696.2022.00013},
  booktitle={2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)},
  pages={29--36},
  year={2022},
  organization={IEEE}
}
